# -*- coding: utf-8 -*-
"""Sectorwise analysis heatmap.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BPxrYYiZbr2w-mayCi65g8XPOVw-HJnB

Updated code (Resolves the consolidation issue)
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import os

# Set a large display limit for Pandas DataFrames for diagnostics
pd.set_option('display.max_rows', 100)
pd.set_option('display.max_columns', 100)

# --- CONFIGURATION ---
PARQUET_FILE = 'all_similarities.paraquet'
CSV_FILE = 'qp-listings.csv'
OUTPUT_HEATMAP = 'sector_similarity_heatmap_final.png'
OUTPUT_TABLE = 'sector_similarity_table_final.csv'

# --- DATA LOADING AND CLEANING ---

# 1. Load the datasets, handling potential filename variations
try:
    df_sim = pd.read_parquet(PARQUET_FILE)
except FileNotFoundError:
    df_sim = pd.read_parquet('all_similarities.parquet')

df_listings = pd.read_csv(CSV_FILE)

# 2. Standardize a specific sector name to ensure consistent grouping
target_name = "Oï¬ƒce Administration & Facility Management"
new_name = "Administration & Facility Management"
df_listings['Sector Name'] = df_listings['Sector Name'].replace(target_name, new_name)

# 3. Calculate the combined average similarity score
# --- UPDATED CALCULATION LOGIC ---
# The 'embedding_similarity' is first multiplied by 100, and then averaged with the other two scores.

scaled_embedding = df_sim['embedding_similarity'] * 100

# Calculate the average of the three scores using the scaled embedding value
df_sim['average_similarity'] = (scaled_embedding + df_sim['task_similarity'] + df_sim['nsqf_level_similarity']) / 3

# --- END UPDATED CALCULATION LOGIC ---

# --- QP CODE TO SECTOR MAPPING (The Waterfall Logic) ---

# Function to generate the versioned code key (e.g., TSC_Q2301_4.0)
def generate_versioned_key(row):
    """Creates the 'qpCode_version.0' format for advanced matching."""
    code = str(row['qpCode'])
    version = str(row['qp_version'])
    if '.' not in version:
        version = version + ".0"
    return f"{code}_{version}"

# Create all three potential matching keys in the listing data
df_listings['match_key_1'] = df_listings['qpCode']
df_listings['match_key_2'] = df_listings.apply(generate_versioned_key, axis=1)
df_listings['match_key_3'] = df_listings['matched_filename'].astype(str).str.replace('.json', '', regex=False)

# Build the comprehensive master mapping dictionary
map_1 = df_listings[['match_key_1', 'Sector Name']].rename(columns={'match_key_1': 'join_key'})
map_2 = df_listings[['match_key_2', 'Sector Name']].rename(columns={'match_key_2': 'join_key'})
map_3 = df_listings[['match_key_3', 'Sector Name']].rename(columns={'match_key_3': 'join_key'})

master_map = pd.concat([map_1, map_2, map_3])
master_map = master_map.dropna(subset=['join_key'])
master_map = master_map[master_map['join_key'] != 'nan']
master_map = master_map.drop_duplicates(subset=['join_key'])

# --- MERGING AND CONSOLIDATION ---

# Merge to get the raw Sector names for both QP codes
df_merged = df_sim.merge(master_map, left_on='qp1_code', right_on='join_key', how='left')
df_merged = df_merged.rename(columns={'Sector Name': 'RawSector1'}).drop(columns=['join_key'])

df_merged = df_merged.merge(master_map, left_on='qp2_code', right_on='join_key', how='left')
df_merged = df_merged.rename(columns={'Sector Name': 'RawSector2'}).drop(columns=['join_key'])

# Filter out rows where sector mapping failed
df_clean = df_merged.dropna(subset=['RawSector1', 'RawSector2']).copy()

def consolidate_sectors(row):
    """Sorts the two sector names alphabetically to create a canonical pair key.
    This ensures that (A, B) and (B, A) are treated as the same pair for aggregation."""
    s1 = row['RawSector1']
    s2 = row['RawSector2']
    # Use tuple sorting for reliable comparison
    return pd.Series(sorted((s1, s2)), index=['Sector1', 'Sector2'])

# Apply the consolidation logic
df_clean[['Sector1', 'Sector2']] = df_clean.apply(consolidate_sectors, axis=1)

# --- MATRIX CREATION AND SYMMETRIZATION ---

# 1. Calculate the Consolidated Average
# All (A, B) and (B, A) interactions are now averaged together
heatmap_data = df_clean.groupby(['Sector1', 'Sector2'])['average_similarity'].mean().reset_index()

# 2. Pivot the data to a matrix form
heatmap_matrix = heatmap_data.pivot(index='Sector1', columns='Sector2', values='average_similarity')

# 3. Get the complete list of 45 sectors for consistent axis ordering
all_sectors = sorted(df_listings['Sector Name'].dropna().unique())

# 4. Reindex to ensure a 45x45 matrix size, filling missing pairs (which did not interact) with NaN
heatmap_matrix = heatmap_matrix.reindex(index=all_sectors, columns=all_sectors)

# 5. Mirror the data to make the matrix symmetric for display
# This ensures that Similarity(A, B) = Similarity(B, A)
# The T property (transpose) flips the matrix, and we use fillna to combine it with the original.
# We fill NaN (bottom triangle) with the values from the transposed upper triangle.
symmetric_matrix = heatmap_matrix.fillna(0) + heatmap_matrix.T.fillna(0)

# Final Symmetrization: Fill the lower triangle with the upper triangle values
final_matrix = symmetric_matrix.mask(symmetric_matrix == 0, heatmap_matrix)
final_matrix = final_matrix.fillna(0)

# 6. Apply self-similarity = 1.0 assumption for the diagonal if not provided by data
# This ensures that a sector compared to itself is marked as 1.0 (or the maximum expected similarity)
np.fill_diagonal(final_matrix.values, np.diag(final_matrix.values) + (1.0 - np.diag(final_matrix.values)))

# --- HEATMAP GENERATION ---

plt.figure(figsize=(24, 24))

# Create mask for the LOWER triangle (k=-1 keeps the diagonal visible, masking everything below it)
# This results in the desired Top-Right triangular display
mask = np.tril(np.ones_like(final_matrix, dtype=bool), k=-1)

sns.heatmap(final_matrix, mask=mask, annot=True, fmt=".2f", cmap='coolwarm',
            linewidths=.5, cbar_kws={'label': 'Average Similarity Score'},
            annot_kws={"size": 8})

plt.title('Average Consolidated Similarity Between Sectors', fontsize=20)
plt.xlabel('Sector 2', fontsize=14)
plt.ylabel('Sector 1', fontsize=14)
plt.xticks(rotation=90, fontsize=10)
plt.yticks(rotation=0, fontsize=10)
plt.tight_layout()

plt.savefig(OUTPUT_HEATMAP)
plt.show()

# --- SAVE TABLE (Unconsolidated format for easy review) ---

# The heatmap_data already contains the consolidated pairs (A, B where A <= B)
# We use this as the primary table output
similarity_table = heatmap_data.sort_values(by='average_similarity', ascending=False)
similarity_table.to_csv(OUTPUT_TABLE, index=False)

print(f"Heatmap generated: {OUTPUT_HEATMAP}")
print(f"Consolidated table generated: {OUTPUT_TABLE}")

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# --- CONFIGURATION ---
OUTPUT_HEATMAP = 'sector_similarity_heatmap_8_sectors.png'
OUTPUT_TABLE = 'sector_similarity_table_8_sectors.csv'

# 1. Load the datasets
try:
    df_sim = pd.read_parquet('all_similarities.paraquet')
except FileNotFoundError:
    df_sim = pd.read_parquet('all_similarities.parquet')

df_listings = pd.read_csv('qp-listings.csv')

# 2. Define the Target Sectors (The 8 specific ones)
target_sectors = [
    "Automotive",
    "Food Processing",
    "Green Jobs",
    "Handicrafts and Carpet",
    "Iron and Steel",
    "Mining",
    "Textile",
    "Tourism & Hospitality"
]
# Ensure the list is sorted alphabetically for consistent plotting
target_sectors = sorted(target_sectors)

# 3. Pre-processing & Data Cleaning

# The 'embedding_similarity' is first multiplied by 100, and then averaged with the other two scores.
scaled_embedding = df_sim['embedding_similarity'] * 100
df_sim['average_similarity'] = (scaled_embedding + df_sim['task_similarity'] + df_sim['nsqf_level_similarity']) / 3
# --- END UPDATED CALCULATION LOGIC ---


# Fix Sector Name in CSV (Standard procedure)
target_name = "Oï¬ƒce Administration & Facility Management"
new_name = "Administration & Facility Management"
df_listings['Sector Name'] = df_listings['Sector Name'].replace(target_name, new_name)

# 4. Build the Master Lookup (Waterfall Matching Logic)
# Match Key 1: Original Code
df_listings['match_key_1'] = df_listings['qpCode']

# Match Key 2: Versioned Code (e.g., Code_4.0)
def format_version_key(row):
    """Creates the 'qpCode_version.0' format for advanced matching."""
    code = str(row['qpCode'])
    version = str(row['qp_version'])
    if '.' not in version:
        version = version + ".0"
    return f"{code}_{version}"
df_listings['match_key_2'] = df_listings.apply(format_version_key, axis=1)

# Match Key 3: Filename (minus .json)
df_listings['match_key_3'] = df_listings['matched_filename'].astype(str).str.replace('.json', '', regex=False)

# Stack mappings
map_1 = df_listings[['match_key_1', 'Sector Name']].rename(columns={'match_key_1': 'join_key'})
map_2 = df_listings[['match_key_2', 'Sector Name']].rename(columns={'match_key_2': 'join_key'})
map_3 = df_listings[['match_key_3', 'Sector Name']].rename(columns={'match_key_3': 'join_key'})

master_map = pd.concat([map_1, map_2, map_3])
master_map = master_map.dropna(subset=['join_key'])
master_map = master_map[master_map['join_key'] != 'nan']
master_map = master_map.drop_duplicates(subset=['join_key'])

# 5. Merge Data
# Merge to get the raw Sector names for both QP codes
df_merged = df_sim.merge(master_map, left_on='qp1_code', right_on='join_key', how='left')
df_merged = df_merged.rename(columns={'Sector Name': 'RawSector1'}).drop(columns=['join_key'])

df_merged = df_merged.merge(master_map, left_on='qp2_code', right_on='join_key', how='left')
df_merged = df_merged.rename(columns={'Sector Name': 'RawSector2'}).drop(columns=['join_key'])

# Filter out rows where sector mapping failed
df_clean = df_merged.dropna(subset=['RawSector1', 'RawSector2']).copy()

# 6. Consolidation Logic
def consolidate_sectors(row):
    """Sorts the two sector names alphabetically to create a canonical pair key.
    This ensures that (A, B) and (B, A) are treated as the same pair for aggregation."""
    s1 = row['RawSector1']
    s2 = row['RawSector2']
    # Use tuple sorting for reliable comparison
    return pd.Series(sorted((s1, s2)), index=['Sector1', 'Sector2'])

# Apply the consolidation and filter for only the 8 target sectors
df_clean[['Sector1', 'Sector2']] = df_clean.apply(consolidate_sectors, axis=1)

# Filter for ONLY the 8 Target Sectors in the consolidated columns
df_subset = df_clean[
    (df_clean['Sector1'].isin(target_sectors)) &
    (df_clean['Sector2'].isin(target_sectors))
]

# 7. Create Matrix (Symmetrized)

# 1. Calculate the Consolidated Average
heatmap_data = df_subset.groupby(['Sector1', 'Sector2'])['average_similarity'].mean().reset_index()

# 2. Pivot the data to a matrix form
# This creates a matrix where data only exists for A <= B
heatmap_matrix = heatmap_data.pivot(index='Sector1', columns='Sector2', values='average_similarity')

# 3. Reindex to force the 8x8 matrix size
heatmap_matrix = heatmap_matrix.reindex(index=target_sectors, columns=target_sectors)

# 4. Final Symmetrization for Display: Fill NaNs in the lower triangle with 0
final_matrix = heatmap_matrix.fillna(0)

# 5. Apply self-similarity = 1.0 assumption for the diagonal
np.fill_diagonal(final_matrix.values, np.diag(final_matrix.values) + (1.0 - np.diag(final_matrix.values)))

# 8. Generate Heatmap
plt.figure(figsize=(10, 8)) # Appropriate size for 8x8

# Mask the LOWER triangle (Top-Right visible)
mask = np.tril(np.ones_like(final_matrix, dtype=bool), k=-1)

sns.heatmap(final_matrix, mask=mask, annot=True, fmt=".2f", cmap='coolwarm',
            linewidths=.5, cbar_kws={'label': 'Average Similarity Score'})

plt.title('Average Consolidated Similarity: Selected 8 Sectors', fontsize=16)
plt.xlabel('Sector 2', fontsize=12)
plt.ylabel('Sector 1', fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()

plt.savefig(OUTPUT_HEATMAP)
plt.show()

# 9. Save Table
# The heatmap_data already contains the consolidated pairs (A, B where A <= B)
similarity_table = heatmap_data.sort_values(by='average_similarity', ascending=False)
similarity_table.to_csv(OUTPUT_TABLE, index=False)

print(f"Heatmap generated: {OUTPUT_HEATMAP}")
print(f"Consolidated table generated: {OUTPUT_TABLE}")

