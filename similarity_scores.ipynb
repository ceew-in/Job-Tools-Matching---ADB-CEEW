{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtDel8QJ5PrE"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This single file includes every function it depends on, so it can run without\n",
        "importing from src/. It mirrors:\n",
        "1. POST /compare-qps  -> `pair` subcommand (skill/task/NSQF/composite scores)\n",
        "2. POST /qp-similarity-scores -> `matrix` subcommand (sector-to-sector matrix)\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import pickle\n",
        "from dataclasses import dataclass, field\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Public API functions that callers can import directly\n",
        "__all__ = [\n",
        "    \"run_pair_command\",\n",
        "    \"run_matrix_command\",\n",
        "    \"get_percentage_match\",\n",
        "    \"get_task_similarity\",\n",
        "    \"get_nsqf_level_similarity\",\n",
        "    \"get_composite_similarity\",\n",
        "    \"score_matrix\",\n",
        "]\n",
        "\n",
        "# ------------------------------------------------------------------------------------\n",
        "# User-facing helpers (defined first so they can be imported easily)\n",
        "# ------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def run_pair_command(lhs_qp_id: str, rhs_qp_id: str) -> dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Compute skill, task, NSQF, and composite similarity scores for two QPs.\n",
        "    Mirrors the /compare-qps endpoint.\n",
        "    \"\"\"\n",
        "    lhs_qp = get_qp_by_id(lhs_qp_id)\n",
        "    rhs_qp = get_qp_by_id(rhs_qp_id)\n",
        "    if not lhs_qp or not rhs_qp:\n",
        "        missing = lhs_qp_id if not lhs_qp else rhs_qp_id\n",
        "        raise ValueError(\n",
        "            f\"QP '{missing}' not found. Ensure data/qp-listings files are available.\"\n",
        "        )\n",
        "\n",
        "    skill_similarity = get_percentage_match(lhs_qp_id, rhs_qp_id)\n",
        "    task_similarity = get_task_similarity(lhs_qp_id, rhs_qp_id)\n",
        "\n",
        "    lhs_level = float(lhs_qp.nsqf_level if lhs_qp.nsqf_level is not None else 5.0)\n",
        "    rhs_level = float(rhs_qp.nsqf_level if rhs_qp.nsqf_level is not None else 5.0)\n",
        "    same_sector = (\n",
        "        lhs_qp.sector.id == rhs_qp.sector.id\n",
        "        if lhs_qp.sector and rhs_qp.sector\n",
        "        else False\n",
        "    )\n",
        "    nsqf_similarity = get_nsqf_level_similarity(lhs_level, rhs_level, same_sector)\n",
        "\n",
        "    composite = get_composite_similarity(\n",
        "        skill_similarity, task_similarity, nsqf_similarity\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"lhs_qp_id\": lhs_qp_id,\n",
        "        \"rhs_qp_id\": rhs_qp_id,\n",
        "        \"similarity_score\": skill_similarity,\n",
        "        \"task_similarity_score\": task_similarity,\n",
        "        \"nsqf_level_similarity_score\": nsqf_similarity,\n",
        "        \"composite_similarity\": composite,\n",
        "    }\n",
        "\n",
        "\n",
        "def run_matrix_command(\n",
        "    sector1: str,\n",
        "    sector2: str,\n",
        "    sub_sector1: Optional[str] = None,\n",
        "    sub_sector2: Optional[str] = None,\n",
        "    occupation1: Optional[str] = None,\n",
        "    occupation2: Optional[str] = None,\n",
        "    filter_levels: Optional[List[str]] = None,\n",
        "    filter_technical: Optional[List[str]] = None,\n",
        ") -> dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Compute the sector-to-sector similarity matrix.\n",
        "    Mirrors the /qp-similarity-scores endpoint.\n",
        "    \"\"\"\n",
        "    return score_matrix(\n",
        "        sector1=sector1,\n",
        "        sector2=sector2,\n",
        "        sub_sector1=sub_sector1,\n",
        "        sub_sector2=sub_sector2,\n",
        "        occupation1=occupation1,\n",
        "        occupation2=occupation2,\n",
        "        filter_levels=filter_levels,\n",
        "        filter_technical=filter_technical,\n",
        "    )\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------------\n",
        "# Data models and loaders (parity with src.qps.utils but embedded for standalone use)\n",
        "# ------------------------------------------------------------------------------------\n",
        "\n",
        "DATA_DIR = Path(\"data\")\n",
        "QP_CSV_PATH = DATA_DIR / \"qp-listings.csv\"\n",
        "TASK_JSON_DIR = DATA_DIR / \"task-jsons\"\n",
        "ALL_L2_SKILLS = DATA_DIR / \"l2_vector_74_esco.json\"\n",
        "SIMILARITY_MATRIX_PATH = DATA_DIR / \"similarities\" / \"qp_similarities.pkl\"\n",
        "\n",
        "SKILL_SIMILARITY_WEIGHTED_AVG = 0.33\n",
        "TASK_SIMILARITY_WEIGHTED_AVG = 0.33\n",
        "NSQF_LEVEL_SIMILARITY_WEIGHTED_AVG = 0.33\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class SubSector:\n",
        "    id: Optional[str]\n",
        "    name: Optional[str]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Sector:\n",
        "    id: Optional[str]\n",
        "    name: Optional[str]\n",
        "    sub_sectors: List[SubSector] = field(default_factory=list)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Occupation:\n",
        "    id: Optional[str]\n",
        "    code: Optional[str]\n",
        "    description: Optional[str]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class QP:\n",
        "    code: str\n",
        "    version: Optional[float]\n",
        "    nsqf_level: Optional[float]\n",
        "    job_role: str\n",
        "    job_role_description: str\n",
        "    sector: Sector\n",
        "    occupation: Occupation\n",
        "    technical: bool\n",
        "    economic_sector: Optional[str]\n",
        "    economic_sector_type: Optional[str]\n",
        "\n",
        "    @property\n",
        "    def _id(self) -> str:\n",
        "        if self.version is not None:\n",
        "            return f\"{self.code.replace('/', '_')}_{self.version}\"\n",
        "        return self.code\n",
        "\n",
        "\n",
        "def _ensure_dict(value) -> Optional[dict]:\n",
        "    if value is None or value == \"\":\n",
        "        return None\n",
        "    if isinstance(value, dict):\n",
        "        return value\n",
        "    try:\n",
        "        parsed = json.loads(value)\n",
        "        if isinstance(parsed, dict):\n",
        "            return parsed\n",
        "    except Exception:\n",
        "        return None\n",
        "    return None\n",
        "\n",
        "\n",
        "def _parse_sector(raw: Optional[str]) -> Sector:\n",
        "    data = _ensure_dict(raw) or {}\n",
        "    sub_sectors_raw = data.get(\"subSectors\") or []\n",
        "    sub_sectors = [\n",
        "        SubSector(\n",
        "            id=str(sub.get(\"subSectorID\")) if sub.get(\"subSectorID\") else None,\n",
        "            name=sub.get(\"subSectorName\"),\n",
        "        )\n",
        "        for sub in sub_sectors_raw\n",
        "    ]\n",
        "    return Sector(\n",
        "        id=str(data.get(\"sectorID\")) if data.get(\"sectorID\") else None,\n",
        "        name=data.get(\"sectorName\"),\n",
        "        sub_sectors=sub_sectors,\n",
        "    )\n",
        "\n",
        "\n",
        "def _parse_occupation(raw: Optional[str]) -> Occupation:\n",
        "    obj = _ensure_dict(raw) or {}\n",
        "    return Occupation(\n",
        "        id=str(obj.get(\"occupationID\")) if obj.get(\"occupationID\") else None,\n",
        "        code=obj.get(\"occupationCode\"),\n",
        "        description=obj.get(\"occupationDesc\"),\n",
        "    )\n",
        "\n",
        "\n",
        "def _parse_param_desc(raw: Optional[str]) -> Optional[str]:\n",
        "    obj = _ensure_dict(raw)\n",
        "    if not obj:\n",
        "        return None\n",
        "    return obj.get(\"paramDesc\")\n",
        "\n",
        "\n",
        "def _parse_technical(raw: Optional[str]) -> bool:\n",
        "    desc = (_parse_param_desc(raw) or \"\").strip().lower()\n",
        "    return desc == \"technical\"\n",
        "\n",
        "\n",
        "def _load_qp_records(csv_path: Path) -> list[dict]:\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df = df.sort_values(by=\"version\", ascending=False)\n",
        "    df = df.drop_duplicates(subset=[\"qpCode\"])\n",
        "    df = df.replace({pd.NA: None, np.nan: None})\n",
        "    df = df[df[\"matched_filename\"].notna()]\n",
        "    df = df[df[\"matched_filename\"] != \"\"]\n",
        "    return [row.to_dict() for _, row in df.iterrows()]\n",
        "\n",
        "\n",
        "def _build_qp_from_row(row: dict) -> QP:\n",
        "    return QP(\n",
        "        code=row.get(\"qpCode\"),\n",
        "        version=row.get(\"version\"),\n",
        "        nsqf_level=row.get(\"nsqfLevel\"),\n",
        "        job_role=row.get(\"jobRole\"),\n",
        "        job_role_description=row.get(\"jobRoleDesc\"),\n",
        "        sector=_parse_sector(row.get(\"sectors\")),\n",
        "        occupation=_parse_occupation(row.get(\"occupation\")),\n",
        "        technical=_parse_technical(row.get(\"qpParamOne\")),\n",
        "        economic_sector=_parse_param_desc(row.get(\"qpParamTwo\")),\n",
        "        economic_sector_type=_parse_param_desc(row.get(\"qpParamThree\")),\n",
        "    )\n",
        "\n",
        "\n",
        "_QP_CACHE: Optional[List[QP]] = None\n",
        "_QP_LOOKUP: Dict[str, QP] = {}\n",
        "\n",
        "\n",
        "def get_all_qps() -> List[QP]:\n",
        "    global _QP_CACHE, _QP_LOOKUP\n",
        "    if _QP_CACHE is None:\n",
        "        qps = [_build_qp_from_row(row) for row in _load_qp_records(QP_CSV_PATH)]\n",
        "        _QP_CACHE = qps\n",
        "        _QP_LOOKUP = {qp._id: qp for qp in qps}\n",
        "    return _QP_CACHE\n",
        "\n",
        "\n",
        "def get_qp_by_id(qp_id: str) -> Optional[QP]:\n",
        "    get_all_qps()\n",
        "    return _QP_LOOKUP.get(qp_id)\n",
        "\n",
        "\n",
        "def get_filtered_qps(\n",
        "    sector: str,\n",
        "    sub_sector: Optional[str] = None,\n",
        "    occupation: Optional[str] = None,\n",
        "    levels: Optional[List[str]] = None,\n",
        "    technical: Optional[List[str]] = None,\n",
        ") -> List[QP]:\n",
        "    qps = [qp for qp in get_all_qps() if qp.sector.id == sector]\n",
        "    if sub_sector:\n",
        "        qps = [\n",
        "            qp\n",
        "            for qp in qps\n",
        "            if qp.sector.sub_sectors\n",
        "            and qp.sector.sub_sectors[0].id == sub_sector\n",
        "        ]\n",
        "    if occupation:\n",
        "        qps = [qp for qp in qps if qp.occupation.id == occupation]\n",
        "    if levels:\n",
        "        qps = [\n",
        "            qp\n",
        "            for qp in qps\n",
        "            if qp.nsqf_level is not None and str(int(qp.nsqf_level)) in levels\n",
        "        ]\n",
        "    if technical:\n",
        "        qps = [\n",
        "            qp\n",
        "            for qp in qps\n",
        "            if qp.technical is not None\n",
        "            and (\"Technical\" if qp.technical else \"Non-Technical\") in technical\n",
        "        ]\n",
        "    return qps\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------------\n",
        "# Task data loading + task similarity logic\n",
        "# ------------------------------------------------------------------------------------\n",
        "\n",
        "_TASK_CACHE: Dict[str, dict] = {}\n",
        "_L2_SKILLS_CACHE: Optional[List[str]] = None\n",
        "\n",
        "\n",
        "def get_qp_task_file_name(qp_id: str) -> str:\n",
        "    parts = qp_id.split(\"_\")\n",
        "    if len(parts) >= 3:\n",
        "        version = \"_\".join(parts[2:])\n",
        "        return f\"{parts[0]}_{parts[1]}_{version}.json\"\n",
        "    return f\"{qp_id}.json\"\n",
        "\n",
        "\n",
        "def get_qp_task_data(qp_id: str) -> dict:\n",
        "    if qp_id in _TASK_CACHE:\n",
        "        return _TASK_CACHE[qp_id]\n",
        "\n",
        "    file_path = TASK_JSON_DIR / get_qp_task_file_name(qp_id)\n",
        "    if not file_path.exists():\n",
        "        _TASK_CACHE[qp_id] = {}\n",
        "        return {}\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as handle:\n",
        "        data = json.load(handle)\n",
        "    _TASK_CACHE[qp_id] = data\n",
        "    return data\n",
        "\n",
        "\n",
        "def calculate_skill_similarity(hierarchy1, hierarchy2, all_l2_skills) -> float:\n",
        "    try:\n",
        "        l1_names1 = {item.get(\"l1\") for item in hierarchy1 if item.get(\"l1\")}\n",
        "        l1_names2 = {item.get(\"l1\") for item in hierarchy2 if item.get(\"l1\")}\n",
        "        all_l1_dimensions = sorted(list(l1_names1.union(l1_names2)))\n",
        "\n",
        "        similarity_l1 = 0.0\n",
        "        if all_l1_dimensions:\n",
        "            l1_dimension_map = {name: i for i, name in enumerate(all_l1_dimensions)}\n",
        "            num_l1_dimensions = len(all_l1_dimensions)\n",
        "            vector1_l1 = np.zeros(num_l1_dimensions)\n",
        "            vector2_l1 = np.zeros(num_l1_dimensions)\n",
        "\n",
        "            for item in hierarchy1:\n",
        "                idx = l1_dimension_map.get(item.get(\"l1\"))\n",
        "                if idx is not None:\n",
        "                    vector1_l1[idx] = item.get(\n",
        "                        \"norm_w_sum_of_all_tasks_under_this_l1\", 0.0\n",
        "                    )\n",
        "\n",
        "            for item in hierarchy2:\n",
        "                idx = l1_dimension_map.get(item.get(\"l1\"))\n",
        "                if idx is not None:\n",
        "                    vector2_l1[idx] = item.get(\n",
        "                        \"norm_w_sum_of_all_tasks_under_this_l1\", 0.0\n",
        "                    )\n",
        "\n",
        "            norm1_l1 = np.linalg.norm(vector1_l1)\n",
        "            norm2_l1 = np.linalg.norm(vector2_l1)\n",
        "\n",
        "            if norm1_l1 > 0 and norm2_l1 > 0:\n",
        "                dot_product_l1 = np.dot(vector1_l1, vector2_l1)\n",
        "                similarity_l1 = dot_product_l1 / (norm1_l1 * norm2_l1)\n",
        "\n",
        "        l2_dimension_map = {skill: i for i, skill in enumerate(all_l2_skills)}\n",
        "        num_l2_dimensions = len(all_l2_skills)\n",
        "        vector1_l2 = np.zeros(num_l2_dimensions)\n",
        "        vector2_l2 = np.zeros(num_l2_dimensions)\n",
        "\n",
        "        for l1_item in hierarchy1:\n",
        "            if \"children\" in l1_item and isinstance(l1_item[\"children\"], list):\n",
        "                for l2_item in l1_item[\"children\"]:\n",
        "                    l2_name = l2_item.get(\"l2\")\n",
        "                    if l2_name in l2_dimension_map:\n",
        "                        idx = l2_dimension_map[l2_name]\n",
        "                        vector1_l2[idx] = l2_item.get(\n",
        "                            \"norm_w_sum_of_all_tasks_under_this_l2\", 0.0\n",
        "                        )\n",
        "\n",
        "        for l1_item in hierarchy2:\n",
        "            if \"children\" in l1_item and isinstance(l1_item[\"children\"], list):\n",
        "                for l2_item in l1_item[\"children\"]:\n",
        "                    l2_name = l2_item.get(\"l2\")\n",
        "                    if l2_name in l2_dimension_map:\n",
        "                        idx = l2_dimension_map[l2_name]\n",
        "                        vector2_l2[idx] = l2_item.get(\n",
        "                            \"norm_w_sum_of_all_tasks_under_this_l2\", 0.0\n",
        "                        )\n",
        "\n",
        "        similarity_l2 = 0.0\n",
        "        norm1_l2 = np.linalg.norm(vector1_l2)\n",
        "        norm2_l2 = np.linalg.norm(vector2_l2)\n",
        "\n",
        "        if norm1_l2 > 0 and norm2_l2 > 0:\n",
        "            dot_product_l2 = np.dot(vector1_l2, vector2_l2)\n",
        "            similarity_l2 = dot_product_l2 / (norm1_l2 * norm2_l2)\n",
        "\n",
        "        final_similarity = ((similarity_l1 + similarity_l2) / 2) * 100\n",
        "        return final_similarity\n",
        "\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "def get_task_similarity(\n",
        "    lhs_qp_id: str,\n",
        "    rhs_qp_id: str,\n",
        "    lhs_hierarchy=None,\n",
        "    rhs_hierarchy=None,\n",
        ") -> float:\n",
        "    global _L2_SKILLS_CACHE\n",
        "\n",
        "    if lhs_hierarchy is None:\n",
        "        lhs_task_data = get_qp_task_data(lhs_qp_id)\n",
        "        lhs_hierarchy = lhs_task_data.get(\"skill_hierarchy\")\n",
        "    if rhs_hierarchy is None:\n",
        "        rhs_task_data = get_qp_task_data(rhs_qp_id)\n",
        "        rhs_hierarchy = rhs_task_data.get(\"skill_hierarchy\")\n",
        "\n",
        "    if not lhs_hierarchy or not rhs_hierarchy:\n",
        "        return 0.0\n",
        "\n",
        "    if _L2_SKILLS_CACHE is None:\n",
        "        if not ALL_L2_SKILLS.exists():\n",
        "            return 0.0\n",
        "        with open(ALL_L2_SKILLS, \"r\", encoding=\"utf-8\") as handle:\n",
        "            _L2_SKILLS_CACHE = json.load(handle)\n",
        "\n",
        "    return calculate_skill_similarity(lhs_hierarchy, rhs_hierarchy, _L2_SKILLS_CACHE)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------------\n",
        "# Similarity matrix handling (rewritten from src.similarity_utils)\n",
        "# ------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class SimilarityMatrix:\n",
        "    def __init__(self, qp_codes: Optional[List[str]] = None):\n",
        "        if qp_codes:\n",
        "            self.qp_codes = qp_codes\n",
        "            self.code_to_index = {code: i for i, code in enumerate(qp_codes)}\n",
        "            self.matrix = np.ones((len(qp_codes), len(qp_codes)))\n",
        "        else:\n",
        "            self.qp_codes = []\n",
        "            self.code_to_index = {}\n",
        "            self.matrix = np.array([])\n",
        "\n",
        "    def normalize_score(self, score: float) -> float:\n",
        "        if score <= 0.0:\n",
        "            return 0.0\n",
        "        min_val = 0.35\n",
        "        max_val = 0.85\n",
        "        score_range = max_val - min_val\n",
        "        normalized_scores = ((score - min_val) / score_range) * 100\n",
        "        normalized_scores_clipped = max(0.0, min(100.0, normalized_scores))\n",
        "        return round(normalized_scores_clipped, 2)\n",
        "\n",
        "    def has_qp_code(self, qp_code: str) -> bool:\n",
        "        return qp_code in self.code_to_index\n",
        "\n",
        "    def get_similarity(self, qp1_code: str, qp2_code: str) -> float:\n",
        "        if qp1_code not in self.code_to_index or qp2_code not in self.code_to_index:\n",
        "            return 0.0\n",
        "        i, j = self.code_to_index[qp1_code], self.code_to_index[qp2_code]\n",
        "        return self.matrix[i, j]\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, filepath: Path) -> \"SimilarityMatrix\":\n",
        "        with open(filepath, \"rb\") as handle:\n",
        "            data = pickle.load(handle)\n",
        "\n",
        "        instance = cls()\n",
        "        if isinstance(data, pd.DataFrame):\n",
        "            instance.qp_codes = data.index.tolist()\n",
        "            instance.code_to_index = {\n",
        "                code: i for i, code in enumerate(instance.qp_codes)\n",
        "            }\n",
        "            instance.matrix = data.values\n",
        "        elif isinstance(data, dict):\n",
        "            instance.qp_codes = data[\"qp_codes\"]\n",
        "            instance.code_to_index = data[\"code_to_index\"]\n",
        "            instance.matrix = data[\"matrix\"]\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Unsupported data format: {type(data)}. Expected DataFrame or dict.\"\n",
        "            )\n",
        "        return instance\n",
        "\n",
        "\n",
        "_SIM_MATRIX: Optional[SimilarityMatrix] = None\n",
        "\n",
        "\n",
        "def _get_similarity_matrix() -> SimilarityMatrix:\n",
        "    global _SIM_MATRIX\n",
        "    if _SIM_MATRIX is None:\n",
        "        if not SIMILARITY_MATRIX_PATH.exists():\n",
        "            raise FileNotFoundError(\n",
        "                f\"Similarity matrix not found at {SIMILARITY_MATRIX_PATH}\"\n",
        "            )\n",
        "        _SIM_MATRIX = SimilarityMatrix.load(SIMILARITY_MATRIX_PATH)\n",
        "    return _SIM_MATRIX\n",
        "\n",
        "\n",
        "def get_percentage_match(lhs_qp_id: str, rhs_qp_id: str) -> float:\n",
        "    sim_matrix = _get_similarity_matrix()\n",
        "    raw_score = sim_matrix.get_similarity(lhs_qp_id, rhs_qp_id)\n",
        "    return sim_matrix.normalize_score(raw_score)\n",
        "\n",
        "\n",
        "def get_nsqf_level_similarity(\n",
        "    job_a_level: float, job_b_level: float, is_same_sector: bool\n",
        ") -> float:\n",
        "    MIN_LEVEL = 1\n",
        "    MAX_LEVEL = 8\n",
        "    BETA = 5.5\n",
        "    MAX_PENALTY = (MAX_LEVEL - MIN_LEVEL) * MAX_LEVEL + BETA * MAX_LEVEL\n",
        "\n",
        "    job_a_level = max(MIN_LEVEL, min(MAX_LEVEL, job_a_level))\n",
        "    job_b_level = max(MIN_LEVEL, min(MAX_LEVEL, job_b_level))\n",
        "\n",
        "    level_penalty = (\n",
        "        (job_b_level - job_a_level) * job_b_level if job_b_level > job_a_level else 0\n",
        "    )\n",
        "    sector_penalty = 0 if is_same_sector else BETA * job_b_level\n",
        "\n",
        "    total_penalty = level_penalty + sector_penalty\n",
        "    score = 100 * (1 - total_penalty / MAX_PENALTY)\n",
        "    return round(max(0.0, min(100.0, score)), 2)\n",
        "\n",
        "\n",
        "def get_composite_similarity(\n",
        "    similarity_score: float,\n",
        "    task_similarity_score: float,\n",
        "    nsqf_level_similarity_score: float,\n",
        ") -> float:\n",
        "    return (\n",
        "        similarity_score * SKILL_SIMILARITY_WEIGHTED_AVG\n",
        "        + task_similarity_score * SKILL_SIMILARITY_WEIGHTED_AVG\n",
        "        + nsqf_level_similarity_score * NSQF_LEVEL_SIMILARITY_WEIGHTED_AVG\n",
        "    )\n",
        "\n",
        "\n",
        "def score_matrix(\n",
        "    sector1: str,\n",
        "    sector2: str,\n",
        "    sub_sector1: Optional[str] = None,\n",
        "    sub_sector2: Optional[str] = None,\n",
        "    occupation1: Optional[str] = None,\n",
        "    occupation2: Optional[str] = None,\n",
        "    filter_levels: Optional[List[str]] = None,\n",
        "    filter_technical: Optional[List[str]] = None,\n",
        ") -> dict:\n",
        "    sim_matrix = _get_similarity_matrix()\n",
        "\n",
        "    qp1s = get_filtered_qps(\n",
        "        sector1, sub_sector1, occupation1, filter_levels, filter_technical\n",
        "    )\n",
        "    qp2s = get_filtered_qps(\n",
        "        sector2, sub_sector2, occupation2, filter_levels, filter_technical\n",
        "    )\n",
        "\n",
        "    qp1s_filtered = [qp for qp in qp1s if sim_matrix.has_qp_code(qp._id)]\n",
        "    qp2s_filtered = [qp for qp in qp2s if sim_matrix.has_qp_code(qp._id)]\n",
        "\n",
        "    qp1_ids = [qp._id for qp in qp1s_filtered]\n",
        "    qp2_ids = [qp._id for qp in qp2s_filtered]\n",
        "\n",
        "    qp_lookup = {qp._id: qp for qp in qp1s_filtered + qp2s_filtered}\n",
        "\n",
        "    task_data_cache = {}\n",
        "    for qp_id in set(qp1_ids + qp2_ids):\n",
        "        task_data = get_qp_task_data(qp_id)\n",
        "        if task_data:\n",
        "            task_data_cache[qp_id] = task_data.get(\"skill_hierarchy\")\n",
        "\n",
        "    scores: List[List[float]] = []\n",
        "\n",
        "    for qp1_id in qp1_ids:\n",
        "        qp_scores = []\n",
        "        lhs_hierarchy = task_data_cache.get(qp1_id)\n",
        "        for qp2_id in qp2_ids:\n",
        "            skill_sim_score = get_percentage_match(qp1_id, qp2_id)\n",
        "            rhs_hierarchy = task_data_cache.get(qp2_id)\n",
        "            task_sim_score = get_task_similarity(\n",
        "                qp1_id, qp2_id, lhs_hierarchy, rhs_hierarchy\n",
        "            )\n",
        "\n",
        "            lhs_qp = qp_lookup.get(qp1_id)\n",
        "            rhs_qp = qp_lookup.get(qp2_id)\n",
        "\n",
        "            nsqf_level_sim_score = 0.0\n",
        "            if lhs_qp and rhs_qp:\n",
        "                lhs_level = (\n",
        "                    lhs_qp.nsqf_level if lhs_qp.nsqf_level is not None else 5.0\n",
        "                )\n",
        "                rhs_level = (\n",
        "                    rhs_qp.nsqf_level if rhs_qp.nsqf_level is not None else 5.0\n",
        "                )\n",
        "                is_same_sector = (\n",
        "                    lhs_qp.sector.id == rhs_qp.sector.id\n",
        "                    if lhs_qp.sector and rhs_qp.sector\n",
        "                    else False\n",
        "                )\n",
        "                nsqf_level_sim_score = get_nsqf_level_similarity(\n",
        "                    float(lhs_level), float(rhs_level), is_same_sector\n",
        "                )\n",
        "\n",
        "            composite_sim_score = (\n",
        "                skill_sim_score * SKILL_SIMILARITY_WEIGHTED_AVG\n",
        "                + task_sim_score * TASK_SIMILARITY_WEIGHTED_AVG\n",
        "                + nsqf_level_sim_score * NSQF_LEVEL_SIMILARITY_WEIGHTED_AVG\n",
        "            )\n",
        "\n",
        "            qp_scores.append(composite_sim_score)\n",
        "        scores.append(qp_scores)\n",
        "\n",
        "    qp_metadata: Dict[str, dict] = {}\n",
        "    for qp in qp1s_filtered + qp2s_filtered:\n",
        "        qp_metadata[qp._id] = {\n",
        "            \"job_role\": qp.job_role,\n",
        "            \"sector_id\": qp.sector.id,\n",
        "            \"sector\": qp.sector.name,\n",
        "            \"sub_sector\": (\n",
        "                qp.sector.sub_sectors[0].name if qp.sector.sub_sectors else None\n",
        "            ),\n",
        "            \"sub_sector_id\": (\n",
        "                qp.sector.sub_sectors[0].id if qp.sector.sub_sectors else None\n",
        "            ),\n",
        "            \"occupation_id\": qp.occupation.id,\n",
        "            \"occupation\": qp.occupation.description,\n",
        "            \"nsqf_level\": qp.nsqf_level,\n",
        "            \"technical\": qp.technical,\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        \"qp1\": qp1_ids,\n",
        "        \"qp2\": qp2_ids,\n",
        "        \"similarity_scores\": scores,\n",
        "        \"metadata\": qp_metadata,\n",
        "    }\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------------\n",
        "# CLI commands (pair & matrix)\n",
        "# ------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def _save_or_print(payload: dict[str, Any], out_path: Path | None) -> None:\n",
        "    if out_path:\n",
        "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        out_path.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n",
        "        print(f\"Wrote results to {out_path}\")\n",
        "    else:\n",
        "        print(json.dumps(payload, indent=2))\n",
        "\n",
        "\n",
        "def build_parser() -> argparse.ArgumentParser:\n",
        "    parser = argparse.ArgumentParser(description=\"QP similarity tooling.\")\n",
        "    parser.add_argument(\n",
        "        \"--output\",\n",
        "        type=Path,\n",
        "        help=\"Optional path to dump the JSON response.\",\n",
        "    )\n",
        "\n",
        "    subparsers = parser.add_subparsers(dest=\"command\", required=True)\n",
        "\n",
        "    pair_parser = subparsers.add_parser(\n",
        "        \"pair\",\n",
        "        help=\"Replicates the /compare-qps API endpoint for two QPs.\",\n",
        "    )\n",
        "    pair_parser.add_argument(\"lhs_qp_id\", help=\"Left-hand QP ID (e.g. AGR_Q0509_1.0).\")\n",
        "    pair_parser.add_argument(\"rhs_qp_id\", help=\"Right-hand QP ID.\")\n",
        "\n",
        "    matrix_parser = subparsers.add_parser(\n",
        "        \"matrix\",\n",
        "        help=\"Replicates the /qp-similarity-scores API endpoint.\",\n",
        "    )\n",
        "    matrix_parser.add_argument(\"--sector1\", required=True, help=\"First sector ID.\")\n",
        "    matrix_parser.add_argument(\"--sector2\", required=True, help=\"Second sector ID.\")\n",
        "    matrix_parser.add_argument(\n",
        "        \"--sub-sector1\", dest=\"sub_sector1\", help=\"Optional sub-sector ID for sector1.\"\n",
        "    )\n",
        "    matrix_parser.add_argument(\n",
        "        \"--sub-sector2\", dest=\"sub_sector2\", help=\"Optional sub-sector ID for sector2.\"\n",
        "    )\n",
        "    matrix_parser.add_argument(\"--occupation1\", help=\"Optional occupation ID for sector1.\")\n",
        "    matrix_parser.add_argument(\"--occupation2\", help=\"Optional occupation ID for sector2.\")\n",
        "    matrix_parser.add_argument(\n",
        "        \"--filter-levels\",\n",
        "        nargs=\"*\",\n",
        "        help=\"Restrict to NSQF levels (space-separated).\",\n",
        "    )\n",
        "    matrix_parser.add_argument(\n",
        "        \"--filter-technical\",\n",
        "        nargs=\"*\",\n",
        "        help='Restrict to Technical/Non-Technical. Example: --filter-technical Technical \"Non-Technical\"',\n",
        "    )\n",
        "\n",
        "    return parser\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    parser = build_parser()\n",
        "    args = parser.parse_args()\n",
        "    if args.command == \"pair\":\n",
        "        payload = run_pair_command(args.lhs_qp_id, args.rhs_qp_id)\n",
        "    elif args.command == \"matrix\":\n",
        "        payload = run_matrix_command(\n",
        "            sector1=args.sector1,\n",
        "            sector2=args.sector2,\n",
        "            sub_sector1=args.sub_sector1,\n",
        "            sub_sector2=args.sub_sector2,\n",
        "            occupation1=args.occupation1,\n",
        "            occupation2=args.occupation2,\n",
        "            filter_levels=args.filter_levels,\n",
        "            filter_technical=args.filter_technical,\n",
        "        )\n",
        "    else:  # pragma: no cover\n",
        "        parser.error(\"Unknown command\")\n",
        "        return\n",
        "\n",
        "    _save_or_print(payload, args.output)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data quality"
      ],
      "metadata": {
        "id": "lugPms9Dao2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import io\n",
        "\n",
        "input_filename = 'qp_listings_current_jobs.csv'\n",
        "output_filename = 'updated_qp_listings_current_jobs.csv'\n",
        "\n",
        "try:\n",
        "    # 'sep=None' with 'engine=python' tells pandas to sniff the separator automatically\n",
        "    df = pd.read_csv(input_filename, sep=None, engine='python')\n",
        "\n",
        "    # Clean column headers: removes leading/trailing spaces (e.g. \" qpParamOne \" -> \"qpParamOne\")\n",
        "    df.columns = df.columns.str.strip()\n",
        "\n",
        "    print(\"Columns detected:\", df.columns.tolist())\n",
        "\n",
        "    if 'qpParamOne' in df.columns:\n",
        "        # Define the cleaning function\n",
        "        def clean_qp_param(value):\n",
        "            try:\n",
        "                if isinstance(value, str) and value.strip().startswith('{'):\n",
        "                    data = json.loads(value)\n",
        "                    return data.get('paramDesc', value)\n",
        "            except (json.JSONDecodeError, TypeError):\n",
        "                pass\n",
        "            return value\n",
        "\n",
        "        # Apply the cleaning\n",
        "        df['qpParamOne'] = df['qpParamOne'].apply(clean_qp_param)\n",
        "\n",
        "        # Save the file (using tab separator to match your original structure)\n",
        "        df.to_csv(output_filename, index=False, sep='\\t')\n",
        "        print(f\"Success! Updated file saved to: {output_filename}\")\n",
        "\n",
        "        # safely print preview\n",
        "        cols_to_show = [c for c in ['qpCode', 'qpParamOne'] if c in df.columns]\n",
        "        print(df[cols_to_show].head())\n",
        "\n",
        "    else:\n",
        "        print(\"\\nERROR: Could not find 'qpParamOne' column even after auto-detection.\")\n",
        "        print(\"Please check if your CSV headers match the spelling exactly.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{input_filename}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5T-n6l2armG",
        "outputId": "cdadd72d-1b45-49b7-b1a8-90cab398ee0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns detected: ['qpCode', 'version', 'nsqfLevel', 'jobRole', 'jobRoleDesc', 'qpParamOne', 'nqrCode', 'qp_code', 'qp_version', 'qp_name', 'qp_path', 'sectorID', 'Sector Name', 'subSectorID', 'subSectorName', 'Correct Sector Matched', 'Tags', 'Subsectors added or not', 'matched_filename']\n",
            "Success! Updated file saved to: updated_qp_listings_current_jobs.csv\n",
            "      qpCode     qpParamOne\n",
            "0  IAS/Q3001      Technical\n",
            "1  HYC/Q9101      Technical\n",
            "2  RSC/Q0831      Technical\n",
            "3  BSC/Q2401  Non-Technical\n",
            "4  ASC/Q1402      Technical\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Setup filenames\n",
        "input_filename = 'updated_qp_listings_current_jobs.csv' # The file from the previous step\n",
        "output_filename = 'duplicate_job_roles_sheet.csv'\n",
        "\n",
        "try:\n",
        "    # Load the file (auto-detects if it uses tabs or commas)\n",
        "    df = pd.read_csv(input_filename, sep=None, engine='python')\n",
        "\n",
        "    # Clean column headers (removes hidden spaces)\n",
        "    df.columns = df.columns.str.strip()\n",
        "\n",
        "    if 'jobRole' in df.columns:\n",
        "        # 2. Find duplicates\n",
        "        # keep=False ensures we capture ALL rows that have a duplicate (e.g., both \"a\" and \"a\")\n",
        "        duplicate_mask = df.duplicated(subset=['jobRole'], keep=False)\n",
        "        df_duplicates = df[duplicate_mask]\n",
        "\n",
        "        # 3. Sort them so identical roles are grouped together\n",
        "        # (e.g., a,a,a, then g,g, then s,s,s)\n",
        "        df_duplicates_sorted = df_duplicates.sort_values(by='jobRole')\n",
        "\n",
        "        # 4. Report statistics\n",
        "        total_dup_rows = len(df_duplicates_sorted)\n",
        "        unique_dup_roles = df_duplicates_sorted['jobRole'].nunique()\n",
        "\n",
        "        print(\"-\" * 30)\n",
        "        print(\"DUPLICATE REPORT\")\n",
        "        print(\"-\" * 30)\n",
        "        print(f\"Total rows shared by duplicates: {total_dup_rows}\")\n",
        "        print(f\"Number of unique Job Roles repeated: {unique_dup_roles}\")\n",
        "\n",
        "        if total_dup_rows > 0:\n",
        "            print(\"\\nMost frequent duplicates:\")\n",
        "            print(df_duplicates_sorted['jobRole'].value_counts().head())\n",
        "\n",
        "        # 5. Save the grouped sheet\n",
        "        # Using sep='\\t' to match your format (change to ',' if you prefer standard CSV)\n",
        "        df_duplicates_sorted.to_csv(output_filename, index=False, sep='\\t')\n",
        "        print(f\"\\nSaved grouped duplicates to: {output_filename}\")\n",
        "\n",
        "    else:\n",
        "        print(\"Error: Column 'jobRole' not found in the CSV.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{input_filename}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_VLyfTfasEp",
        "outputId": "c0194a9c-7f5b-44b4-db46-b9104e4ad055"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "DUPLICATE REPORT\n",
            "------------------------------\n",
            "Total rows shared by duplicates: 515\n",
            "Number of unique Job Roles repeated: 257\n",
            "\n",
            "Most frequent duplicates:\n",
            "jobRole\n",
            "Water Pump Operator                                           3\n",
            "Junior Instrumentation Technician (Process Control)           2\n",
            "Junior Mechanic (Electrical/ Electronics/ Instrumentation)    2\n",
            "Junior Mechanic (Engine)                                      2\n",
            "Junior Rubber Technician/Technical Assistant (Rubber)         2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Saved grouped duplicates to: duplicate_job_roles_sheet.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Load the parquet file\n",
        "file_path = \"all_similarities_final.parquet\"\n",
        "try:\n",
        "    df = pd.read_parquet(file_path)\n",
        "\n",
        "    # 2. Apply the filter mentioned in the feedback\n",
        "    # \"rows with duplicate job roles (i.e., same job role name in 1 and 2)\"\n",
        "    same_role_mask = df['jobRole_1'] == df['jobRole_2']\n",
        "\n",
        "    # 3. Refine the filter to find the \"confusing\" cases\n",
        "    # The feedback says: \"Although their QP codes are different, everything else is same.\"\n",
        "    # So we filter for Same Role AND Different QP Code\n",
        "    diff_qp_mask = df['qp_code_1'] != df['qp_code_2']\n",
        "\n",
        "    # Combine masks to get the target rows\n",
        "    target_rows = df[same_role_mask & diff_qp_mask]\n",
        "\n",
        "    print(f\"Total rows with same Job Role: {same_role_mask.sum()}\")\n",
        "    print(f\"Rows with same Job Role but DIFFERENT QP Code: {len(target_rows)}\")\n",
        "\n",
        "    # 4. Save the result to a CSV file for manual inspection\n",
        "    output_file = \"duplicate_roles_diff_qp.csv\"\n",
        "    target_rows.to_csv(output_file, index=False)\n",
        "    print(f\"\\nFiltered rows saved to: {output_file}\")\n",
        "\n",
        "    # 5. Quick Analysis: Check if 'everything else' is actually the same\n",
        "    # We can inspect a few columns to see where the differences lie\n",
        "    if not target_rows.empty:\n",
        "        print(\"\\nSample of QP Code pairs for these duplicates:\")\n",
        "        print(target_rows[['qp_code_1', 'qp_code_2', 'jobRole_1']].head(10))\n",
        "\n",
        "        # Check if NSQF Levels are also same\n",
        "        nsqf_diff = target_rows[target_rows['nsqfLevel_1'] != target_rows['nsqfLevel_2']]\n",
        "        print(f\"\\nNumber of these rows where NSQF Level is DIFFERENT: {len(nsqf_diff)}\")\n",
        "\n",
        "        # Check if Sector Names are also same\n",
        "        sector_diff = target_rows[target_rows['Sector Name_1'] != target_rows['Sector Name_2']]\n",
        "        print(f\"Number of these rows where Sector Name is DIFFERENT: {len(sector_diff)}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{file_path}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OElllXijfz0",
        "outputId": "2eebc4b1-48b5-4dff-f5b4-9e1ef4f164d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows with same Job Role: 259\n",
            "Rows with same Job Role but DIFFERENT QP Code: 259\n",
            "\n",
            "Filtered rows saved to: duplicate_roles_diff_qp.csv\n",
            "\n",
            "Sample of QP Code pairs for these duplicates:\n",
            "       qp_code_1                      qp_code_2  \\\n",
            "6662   RSC/Q0831          2022/RUB/RCPSDC/06938   \n",
            "13065  ASC/Q3503            2022/AUT/ASDC/06567   \n",
            "15785  ASC/Q1001    QG-04-AU-03592-2025-V2-ASDC   \n",
            "19854  ISC/Q0904  QG-03-IS-03888-2025-V2-IISSSC   \n",
            "22435  ASC/Q3103            2022/AUT/ASDC/06569   \n",
            "24382  ISC/Q0906  QG-03-IS-03885-2025-V2-IISSSC   \n",
            "36127  PSC/Q0102    QG-03-PL-03440-2024-V2-WMPS   \n",
            "38273  PSC/Q0104    QG-04-PL-03441-2024-V2-WMPS   \n",
            "47337  THC/Q0109   QG-4.5-TH-02014-2024-V1-THSC   \n",
            "49210  ISC/Q0410  QG-02-IS-03876-2025-V2-IISSSC   \n",
            "\n",
            "                                               jobRole_1  \n",
            "6662   Junior Rubber Technician/Technical Assistant (...  \n",
            "13065                Automotive CNC Machining Technician  \n",
            "15785                         Automotive Sales Executive  \n",
            "19854                             Belt Conveyor Mechanic  \n",
            "22435              Automotive Welding Machine Technician  \n",
            "24382             Assistant Bearing Maintenance Mechanic  \n",
            "36127                        Assistant Plumber - General  \n",
            "38273                                  Plumber - General  \n",
            "47337             Guest Service Executive (Front Office)  \n",
            "49210            Helper - Plant Operations & Maintenance  \n",
            "\n",
            "Number of these rows where NSQF Level is DIFFERENT: 8\n",
            "Number of these rows where Sector Name is DIFFERENT: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# 1. Load the datasets\n",
        "# Replace with the actual paths if they are in a specific folder in your Drive\n",
        "df_a = pd.read_csv('adb-share-intermediate.csv')\n",
        "df_b = pd.read_csv('qp_listings_current_jobs.csv')\n",
        "\n",
        "# Optional: Ensure the key column 'qp_code' is the same data type in both (e.g., string)\n",
        "# This prevents errors if one is read as a number and the other as text\n",
        "df_a['qp_code'] = df_a['qp_code'].astype(str)\n",
        "df_b['qp_code'] = df_b['qp_code'].astype(str)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 1: Merge columns from 'a' into 'b'\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# We filter df_a to only keep the columns we need + the joining key\n",
        "cols_to_fetch = ['qp_code', 'Source', 'skill_hierarchy']\n",
        "subset_a = df_a[cols_to_fetch]\n",
        "\n",
        "# Perform a LEFT merge.\n",
        "# This keeps all rows from 'b' and adds matching info from 'a'.\n",
        "merged_df = pd.merge(df_b, subset_a, on='qp_code', how='left')\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 2: Update and Rename 'qpParamOne'\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def extract_param_desc(row_data):\n",
        "    \"\"\"\n",
        "    Parses the JSON string and extracts the 'paramDesc' value.\n",
        "    Returns 'Unknown' or the original value if parsing fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if data is valid\n",
        "        if pd.isna(row_data):\n",
        "            return None\n",
        "\n",
        "        # If it's already a dictionary (unlikely in CSV but possible in some formats)\n",
        "        if isinstance(row_data, dict):\n",
        "            return row_data.get('paramDesc')\n",
        "\n",
        "        # Parse the string as JSON\n",
        "        data_dict = json.loads(row_data)\n",
        "        return data_dict.get('paramDesc')\n",
        "\n",
        "    except (json.JSONDecodeError, AttributeError):\n",
        "        # Fallback if the string is malformed\n",
        "        return row_data\n",
        "\n",
        "# Apply the function to the column\n",
        "merged_df['qpParamOne'] = merged_df['qpParamOne'].apply(extract_param_desc)\n",
        "\n",
        "# Rename the column\n",
        "merged_df.rename(columns={'qpParamOne': 'Technical/Non-Technical'}, inplace=True)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 3: Save the result\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# Display the first few rows to verify\n",
        "print(\"Preview of updated data:\")\n",
        "print(merged_df[['qp_code', 'Technical/Non-Technical', 'Source', 'skill_hierarchy']].head())\n",
        "\n",
        "# Save to a new CSV file\n",
        "merged_df.to_csv('qp_listings_updated.csv', index=False)\n",
        "print(\"\\nSuccess! File saved as 'qp_listings_updated.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0ump5cg_VoW",
        "outputId": "1c096cef-5193-4968-cc01-5ef6acba66b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preview of updated data:\n",
            "     qp_code Technical/Non-Technical Source  \\\n",
            "0  IAS_Q3001               Technical   NSQP   \n",
            "1  HYC_Q9101               Technical   NSQP   \n",
            "2  RSC_Q0831               Technical   NSQP   \n",
            "3  BSC_Q2401           Non-Technical   NSQP   \n",
            "4  ASC_Q1402               Technical   NSQP   \n",
            "\n",
            "                                     skill_hierarchy  \n",
            "0  [{\"l1id\": \"m196\", \"norm_w_sum_of_all_tasks_und...  \n",
            "1  [{\"l1id\": \"m120\", \"norm_w_sum_of_all_tasks_und...  \n",
            "2  [{\"l1id\": \"m52\", \"norm_w_sum_of_all_tasks_unde...  \n",
            "3  [{\"l1id\": \"m83\", \"norm_w_sum_of_all_tasks_unde...  \n",
            "4  [{\"l1id\": \"m279\", \"norm_w_sum_of_all_tasks_und...  \n",
            "\n",
            "Success! File saved as 'qp_listings_updated.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Load the files\n",
        "# Load the file we created in the previous step\n",
        "df_b = pd.read_csv('/content/qp_listings_updated.csv')\n",
        "\n",
        "# Load the new file containing 'nco_clean'\n",
        "# Make sure the filename matches exactly what you uploaded\n",
        "df_new_source = pd.read_csv('A_jr_jd_nc_qpc_updated_with_skills.csv')\n",
        "\n",
        "# 2. Prepare the data\n",
        "# Ensure the key column 'qp_code' is the same data type (string) in both\n",
        "df_b['qp_code'] = df_b['qp_code'].astype(str)\n",
        "df_new_source['qp_code'] = df_new_source['qp_code'].astype(str)\n",
        "\n",
        "# 3. Merge the specific column\n",
        "# We create a subset with only 'qp_code' (for matching) and 'nco_clean' (to add)\n",
        "# This prevents accidental duplication of other columns\n",
        "cols_to_fetch = ['qp_code', 'nco_clean']\n",
        "subset_source = df_new_source[cols_to_fetch]\n",
        "\n",
        "# Perform a LEFT merge to keep all rows from your listings file\n",
        "# and just add the matching nco_clean data\n",
        "df_final = pd.merge(df_b, subset_source, on='qp_code', how='left')\n",
        "\n",
        "# 4. Save the final result\n",
        "print(\"Merge successful. Preview of new column:\")\n",
        "print(df_final[['qp_code', 'nco_clean']].head())\n",
        "\n",
        "# Save to a new csv\n",
        "df_final.to_csv('qp_listings_final_with_nco.csv', index=False)\n",
        "print(\"\\nFile saved as 'qp_listings_final_with_nco.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4Esssn1Nb17",
        "outputId": "e5311fe6-8dcc-4bdd-b805-9f1247c7dda8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merge successful. Preview of new column:\n",
            "     qp_code nco_clean\n",
            "0  IAS_Q3001   3111.99\n",
            "1  HYC_Q9101   7212.03\n",
            "2  RSC_Q0831   4322.02\n",
            "3  BSC_Q2401   3312.01\n",
            "4  ASC_Q1402   3115.06\n",
            "\n",
            "File saved as 'qp_listings_final_with_nco.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dDXmXCGVTQ2-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}