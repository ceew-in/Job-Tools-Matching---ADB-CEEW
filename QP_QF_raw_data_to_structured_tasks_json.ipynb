{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0oXNyHZYoTW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import sys\n",
        "import time\n",
        "import subprocess\n",
        "from collections import defaultdict\n",
        "\n",
        "# --- 0. Pre-flight Checks and Installations ---\n",
        "def install_if_missing(package, import_name=None):\n",
        "    \"\"\"Installs a package using pip if it's not already installed.\"\"\"\n",
        "    if not import_name:\n",
        "        import_name = package\n",
        "    try:\n",
        "        __import__(import_name)\n",
        "    except ImportError:\n",
        "        print(f\"Installing {package}...\")\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
        "            print(f\"‚úÖ {package} installed successfully.\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"üõë Failed to install {package}. Please install it manually. Error: {e}\")\n",
        "            raise\n",
        "\n",
        "# Install all required dependencies quietly\n",
        "install_if_missing(\"google-generativeai\", \"google.generativeai\")\n",
        "install_if_missing(\"sentence-transformers\")\n",
        "install_if_missing(\"scikit-learn\", \"sklearn\")\n",
        "install_if_missing(\"tqdm\")\n",
        "\n",
        "import numpy as np\n",
        "import google.generativeai as genai\n",
        "from google.api_core import exceptions\n",
        "from sentence_transformers import CrossEncoder\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from google.colab import drive, userdata\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- 1. Core Processing Functions (from previous step) ---\n",
        "\n",
        "def setup_environment(gemini_api_key: str, esco_hierarchy_path: str):\n",
        "    \"\"\"\n",
        "    Loads all models, API clients, and reference data.\n",
        "    This should be run only once per session.\n",
        "    \"\"\"\n",
        "    print(\"--- üöÄ Starting Environment Setup ---\")\n",
        "    assets = {}\n",
        "\n",
        "    # 1.1 Configure Gemini API\n",
        "    print(\"1. Configuring Gemini API...\")\n",
        "    try:\n",
        "        genai.configure(api_key=gemini_api_key)\n",
        "        assets['gemini_pos_model'] = genai.GenerativeModel('models/gemini-2.5-flash')\n",
        "        print(\"   ‚úÖ Gemini API configured successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"   üõë ERROR: Could not configure Gemini API: {e}\")\n",
        "        return None\n",
        "\n",
        "    # 1.2 Load Ranking Model\n",
        "    print(\"2. Loading CrossEncoder ranking model...\")\n",
        "    try:\n",
        "        assets['ranking_model'] = CrossEncoder(\"vaibhav-ceew/onet-msmacroL6\")\n",
        "        print(\"   ‚úÖ Ranking model loaded.\")\n",
        "    except Exception as e:\n",
        "        print(f\"   üõë ERROR: Could not load ranking model: {e}\")\n",
        "        return None\n",
        "\n",
        "    # 1.3 Load and Process ESCO Skill Hierarchy\n",
        "    print(\"3. Loading and processing ESCO skill hierarchy...\")\n",
        "    try:\n",
        "        with open(esco_hierarchy_path, 'r', encoding='utf-8') as f:\n",
        "            esco_skills = json.load(f)\n",
        "\n",
        "        skill_map = {skill['id']: skill for skill in esco_skills}\n",
        "        assets['skill_map'] = skill_map\n",
        "        l3_skills = [s for s in esco_skills if s['level'] == 3]\n",
        "        assets['l3_skill_list'] = [s['levelName'] for s in l3_skills]\n",
        "        assets['l3_id_list'] = [s['id'] for s in l3_skills]\n",
        "        assets['all_l1_ids'] = sorted([s['id'] for s in esco_skills if s['level'] == 1])\n",
        "        print(f\"   ‚úÖ Loaded {len(esco_skills)} skills. Found {len(assets['l3_skill_list'])} L3 skills.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   üõë ERROR: Could not process ESCO hierarchy file '{esco_hierarchy_path}': {e}\")\n",
        "        return None\n",
        "\n",
        "    # 1.4 Generate Embeddings for all L3 skills\n",
        "    print(\"4. Generating embeddings for all L3 skills (this may take a while)...\")\n",
        "    l3_skill_embeddings = []\n",
        "    batch_size = 90\n",
        "    l3_list = assets['l3_skill_list']\n",
        "\n",
        "    try:\n",
        "        with tqdm(total=len(l3_list), desc=\"Embedding L3 Skills\") as pbar:\n",
        "            for i in range(0, len(l3_list), batch_size):\n",
        "                batch = l3_list[i:i + batch_size]\n",
        "                time.sleep(1) # Respect API rate limits\n",
        "                result = genai.embed_content(\n",
        "                    model=\"models/embedding-001\",\n",
        "                    content=batch,\n",
        "                    task_type=\"RETRIEVAL_DOCUMENT\"\n",
        "                )\n",
        "                l3_skill_embeddings.extend(result['embedding'])\n",
        "                pbar.update(len(batch))\n",
        "    except Exception as e:\n",
        "        print(f\"   üõë An API error occurred during embedding generation: {e}\")\n",
        "        return None\n",
        "\n",
        "    assets['l3_skill_embeddings'] = np.array(l3_skill_embeddings)\n",
        "    print(f\"   ‚úÖ Generated {len(l3_skill_embeddings)} embeddings.\")\n",
        "\n",
        "    print(\"--- ‚úÖ Environment Setup Complete ---\")\n",
        "    return assets\n",
        "\n",
        "def _get_pos_from_task(task_text, model):\n",
        "    \"\"\"Calls Gemini to transform a task into a structured statement.\"\"\"\n",
        "    prompt = f\"\"\"Transform the sentence \"{task_text}\" into a simple Root verb + Object + Context/Condition structure, eliminating any unnecessary information. If more than two verbs, keep the most important one. Don't use short forms by your own, be precise, specific, and concise.\"\"\"\n",
        "    max_retries = 3\n",
        "    delay = 2\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            time.sleep(0.5) # Small delay to avoid hitting rate limits too fast\n",
        "            response = model.generate_content(prompt)\n",
        "            return response.text.strip()\n",
        "        except Exception:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(delay)\n",
        "                delay *= 2\n",
        "            else:\n",
        "                return \"Error: Transformation Failed\"\n",
        "    return \"Error: Transformation Failed after all retries\"\n",
        "\n",
        "def process_job_profile(job_data: dict, assets: dict):\n",
        "    \"\"\"\n",
        "    Processes a single job JSON object through the entire pipeline.\n",
        "    \"\"\"\n",
        "    processed_data = json.loads(json.dumps(job_data))\n",
        "    original_tasks = processed_data.get(\"tasks\", [])\n",
        "\n",
        "    if not original_tasks or not isinstance(original_tasks, list):\n",
        "        processed_data['tasks'] = []\n",
        "        processed_data['skill_hierarchy'] = []\n",
        "        return processed_data\n",
        "\n",
        "    # Part 1: Task Re-ranking and Weighting\n",
        "    job_context = f\"{processed_data.get('JOB TITLE', '')} {processed_data.get('JOB_DESCRIPTION', '')}\"\n",
        "    model_inputs = [(job_context, task) for task in original_tasks]\n",
        "    scores = assets['ranking_model'].predict(model_inputs, show_progress_bar=False)\n",
        "    scored_tasks = sorted(zip(original_tasks, scores), key=lambda x: x[1], reverse=True)\n",
        "    top_tasks = scored_tasks[:40]\n",
        "    top_scores = [float(score) for _, score in top_tasks]\n",
        "    total_score = sum(top_scores)\n",
        "    processed_tasks = []\n",
        "    if total_score > 0:\n",
        "        for task, score in top_tasks:\n",
        "            processed_tasks.append({\"task\": task, \"norm_w\": float(score) / total_score})\n",
        "\n",
        "    # Part 2, 3, 4: POS, Skill Matching, and Hierarchy Mapping\n",
        "    skill_map = assets['skill_map']\n",
        "    for task_obj in processed_tasks:\n",
        "        task_obj['pos'] = _get_pos_from_task(task_obj['task'], assets['gemini_pos_model'])\n",
        "        if \"Error\" in task_obj['pos']:\n",
        "            task_obj['l1_id'], task_obj['l2_id'], task_obj['l3'] = None, None, []\n",
        "            continue\n",
        "        try:\n",
        "            task_embedding_result = genai.embed_content(\n",
        "                model=\"models/embedding-001\",\n",
        "                content=task_obj['pos'],\n",
        "                task_type=\"RETRIEVAL_QUERY\"\n",
        "            )\n",
        "            task_embedding = task_embedding_result['embedding']\n",
        "            similarities = cosine_similarity([task_embedding], assets['l3_skill_embeddings'])[0]\n",
        "            best_idx = np.argmax(similarities)\n",
        "            l3_id, l3_skill, cos_sim = assets['l3_id_list'][best_idx], assets['l3_skill_list'][best_idx], round(float(similarities[best_idx]), 4)\n",
        "            task_obj['l3'] = [{\"id\": l3_id, \"l3\": l3_skill}, {\"cos_sim\": cos_sim}]\n",
        "            l3_node = skill_map.get(l3_id)\n",
        "            if l3_node and l3_node.get('parentId'):\n",
        "                l2_id = l3_node['parentId']\n",
        "                l2_node = skill_map.get(l2_id)\n",
        "                if l2_node and l2_node.get('parentId'):\n",
        "                    task_obj['l2_id'] = l2_id\n",
        "                    task_obj['l1_id'] = l2_node['parentId']\n",
        "        except Exception:\n",
        "            task_obj['l1_id'], task_obj['l2_id'], task_obj['l3'] = None, None, []\n",
        "\n",
        "    processed_data['tasks'] = processed_tasks\n",
        "\n",
        "    # Part 5: Create Skill Hierarchy\n",
        "    hierarchy = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
        "    for task in processed_tasks:\n",
        "        if task.get('l1_id') and task.get('l2_id') and task.get('l3'):\n",
        "            hierarchy[task['l1_id']][task['l2_id']][task['l3'][0]['id']].append({\"task\": task['task'], \"norm_w\": task['norm_w']})\n",
        "\n",
        "    final_hierarchy = []\n",
        "    for l1_id in assets['all_l1_ids']:\n",
        "        l1_data, l1_children, l1_total_weight = hierarchy.get(l1_id, {}), [], 0\n",
        "        for l2_id, l2_data in l1_data.items():\n",
        "            l2_children, l2_total_weight = [], 0\n",
        "            for l3_id, l3_tasks in l2_data.items():\n",
        "                l3_total_weight = sum(t['norm_w'] for t in l3_tasks)\n",
        "                l2_total_weight += l3_total_weight\n",
        "                l2_children.append({\"l3id\": l3_id, \"norm_w_sum_of_all_tasks_under_this_l3\": l3_total_weight, \"tasks\": l3_tasks})\n",
        "            l1_total_weight += l2_total_weight\n",
        "            l1_children.append({\"l2id\": l2_id, \"norm_w_sum_of_all_tasks_under_this_l2\": l2_total_weight, \"children\": l2_children})\n",
        "        final_hierarchy.append({\"l1id\": l1_id, \"norm_w_sum_of_all_tasks_under_this_l1\": l1_total_weight, \"children\": l1_children})\n",
        "\n",
        "    processed_data['skill_hierarchy'] = sorted(final_hierarchy, key=lambda x: x['norm_w_sum_of_all_tasks_under_this_l1'], reverse=True)\n",
        "    return processed_data\n",
        "\n",
        "# --- 2. Main Execution Block for Batch Processing ---\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the batch processing job.\"\"\"\n",
        "\n",
        "    # --- Configuration ---\n",
        "    try:\n",
        "        GEMINI_API_KEY = userdata.get('GEMINI_KEY')\n",
        "    except Exception as e:\n",
        "        print(\"üõë ERROR: Could not retrieve 'GOOGLE_API_KEY' from Colab secrets.\")\n",
        "        print(\"   Please follow the instructions to add the key and restart the runtime.\")\n",
        "        return\n",
        "\n",
        "    ESCO_FILE = \"esco_skill_hierarchy.json\"\n",
        "\n",
        "    # --- Mount Google Drive ---\n",
        "    print(\"\\n--- Mounting Google Drive ---\")\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"‚úÖ Google Drive mounted successfully.\")\n",
        "\n",
        "    # Define Drive paths\n",
        "    base_drive_path = \"/content/drive/My Drive\"\n",
        "    input_folder_path = os.path.join(base_drive_path, \"Processed_JSON_Files_with_Tasks_NQR_new_25sep\")\n",
        "    output_folder_path = os.path.join(base_drive_path, \"dataset_correct_2_adb\")\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(output_folder_path, exist_ok=True)\n",
        "    print(f\"Input folder:  {input_folder_path}\")\n",
        "    print(f\"Output folder: {output_folder_path}\")\n",
        "\n",
        "    # Check for ESCO file\n",
        "    if not os.path.exists(ESCO_FILE):\n",
        "        print(f\"üõë ERROR: The ESCO hierarchy file '{ESCO_FILE}' was not found in the Colab root directory.\")\n",
        "        print(\"   Please upload the file and try again.\")\n",
        "        return\n",
        "\n",
        "    # --- ONE-TIME SETUP ---\n",
        "    processing_assets = setup_environment(\n",
        "        gemini_api_key=GEMINI_API_KEY,\n",
        "        esco_hierarchy_path=ESCO_FILE\n",
        "    )\n",
        "    if not processing_assets:\n",
        "        print(\"üõë Halting execution due to setup failure.\")\n",
        "        return\n",
        "\n",
        "    # --- Identify Files to Process (and skip processed ones) ---\n",
        "    print(\"\\n--- Identifying files to process ---\")\n",
        "    try:\n",
        "        all_input_files = [f for f in os.listdir(input_folder_path) if f.endswith(\".json\")]\n",
        "        processed_files = set(os.listdir(output_folder_path))\n",
        "\n",
        "        files_to_process = [f for f in all_input_files if f not in processed_files]\n",
        "\n",
        "        print(f\"Found {len(all_input_files)} total files in source folder.\")\n",
        "        print(f\"Found {len(processed_files)} already processed files to skip.\")\n",
        "        print(f\"‚û°Ô∏è  Processing {len(files_to_process)} new files.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"üõë ERROR: Input directory not found at '{input_folder_path}'. Please check the path.\")\n",
        "        return\n",
        "\n",
        "    if not files_to_process:\n",
        "        print(\"\\nüéâ All files are already processed. Nothing to do!\")\n",
        "        return\n",
        "\n",
        "    # --- Main Processing Loop ---\n",
        "    print(f\"\\n--- Starting processing loop for {len(files_to_process)} files ---\")\n",
        "\n",
        "    for filename in tqdm(files_to_process, desc=\"Overall Progress\"):\n",
        "        input_filepath = os.path.join(input_folder_path, filename)\n",
        "        output_filepath = os.path.join(output_folder_path, filename)\n",
        "\n",
        "        try:\n",
        "            # 1. Read the input JSON file\n",
        "            with open(input_filepath, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            # 2. Call the main processing function\n",
        "            final_result = process_job_profile(\n",
        "                job_data=data,\n",
        "                assets=processing_assets\n",
        "            )\n",
        "\n",
        "            # 3. Save the output JSON file\n",
        "            with open(output_filepath, 'w', encoding='utf-8') as f:\n",
        "                json.dump(final_result, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"   - ‚ö†Ô∏è Skipping {filename} due to JSON decoding error.\")\n",
        "        except Exception as e:\n",
        "            print(f\"   - üõë An unexpected error occurred with {filename}: {e}. Skipping file.\")\n",
        "\n",
        "    print(\"\\n\\n--- üéâüéâüéâ All new files processed successfully! üéâüéâüéâ ---\")\n",
        "    print(f\"Results are saved in: {output_folder_path}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    }
  ]
}